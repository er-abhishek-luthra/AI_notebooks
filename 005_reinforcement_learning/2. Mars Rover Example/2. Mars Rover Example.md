---
title: Mars Rover Example
tags: [blog]
publish: true
---

# Mars Rover Example

To finish out the reinforcement learning formalism, instead of looking at something as complicated as a helicopter or a robot dog, we can use a simplified example that's loosely inspired by the Mars rover. This module is adapted from the example by Stanford professor Emma Branskill and Jagriti Agrawal, who had actually written code controlling the Mars rover and helped develop this example.

We'll develop reinforcement learning using a simplified example inspired by the Mars rover. In this application, the rover can be in any of six positions, as shown by the six boxes here.

The rover might start off, say, in position four, as shown below:

![Mars Rover Grid Example](_resources/mars-rover-grid-example.png)

The position of the Mars rover is called the **state** in reinforcement learning. I'm going to call these six states:
- state 1,
- state 2,
- state 3,
- state 4,
- state 5,
- and state 6.

So the rover is starting off in **state 4**.

The rover was sent to Mars to carry out different science missions. It can go to different places to use its sensors such as a drill, radar, or spectrometer to analyze rocks or take pictures.

In this example:
- **State 1** has a very interesting surface that scientists would love for the rover to sample.
- **State 6** also has an interesting surface, but not as interesting as state 1.

We are more likely to carry out the science mission at **state 1** than at **state 6**, but state 1 is further away. 

The way we reflect state 1 being more valuable is through the **reward function**:
- **Reward at state 1** = 100
- **Reward at state 6** = 40
- **Rewards at states 2, 3, 4, and 5** = 0

There isn't as much interesting science to be done at states 2, 3, 4, and 5.

On each step, the rover gets to choose one of two actions:
- go **left**, or
- go **right**.

The question is, **what should the rover do?**

In reinforcement learning, we pay a lot of attention to the rewards because that's how we know if the robot is doing well or poorly.

### Example 1: Going Left

Starting from **state 4**:
- It receives a reward of 0.
- Goes left to **state 3**, receives a reward of 0.
- Goes to **state 2**, reward is 0.
- Goes to **state 1**, receives a reward of 100.

When it gets to either state 1 or state 6, the day ends. These are called **terminal states**.

In reinforcement learning, a terminal state means after the agent reaches it, it gets a reward, but then nothing happens after that.

Maybe the rover runs out of fuel or time for the day, so it only gets to enjoy the 100 or 40 reward, but then that's it.

### Example 2: Going Right

Instead of going left, the robot could choose to go **right**:
- From **state 4**, it first gets a reward of 0.
- Moves to **state 5**, reward is 0.
- Moves to **state 6** (terminal), and gets a reward of 40.

Going left and right are the only options.

### Example 3: Mixed Actions

The robot could:
- Start from **state 4** and move **right** to **state 5** (reward 0).
- Then maybe change its mind, move **left**:
  - **state 4**, **state 3**, **state 2** (all rewards 0),
  - finally reaching **state 1** and getting 100.

In this case, the robot is wasting time. This may not be an efficient way to act, but it's a valid choice the algorithm could make.

### Reinforcement Learning Formalism

To summarize:
- At every time step, the robot is in some **state S**.
- It chooses an **action**.
- It enjoys a **reward R(S)** from that state.
- As a result of the action, it moves to a **new state S'**.

Example:
- When the robot is in **state 4** and takes action **left**, it receives a reward **0** (associated with state 4) and moves to **state 3**.

These four elements:
1. **State (S)**
2. **Action (a)**
3. **Reward (R(S))**
4. **Next state (S')**

These are the core elements reinforcement learning algorithms consider when deciding how to act.

Just for clarity, the reward **R(S)** is associated with the current state **S**, not the next state **S'**.

That's the formalism of how a reinforcement learning application works.

In the next module, we'll explore how to specify exactly what we want the reinforcement learning algorithm to do, particularly focusing on the important idea of the **return**.

Let's go on to the next module to see what that means.
