---
title: Random (stochastic) environment - revision
tags: [revision]
publish: false
---

# Random (stochastic) environment

### Key Concepts:
- **Stochastic environment**: Actions may not lead to the same result every time.
- **Expected return**: Average of sum of discounted rewards over many runs.
- **Bellman equation** is modified to incorporate expected value due to randomness in state transitions.

---

### 🚗 Mars Rover Example:

- Command “left”:
  - 90% chance → state 3
  - 10% chance → state 5

- Command “right”:
  - 90% chance → state 5
  - 10% chance → state 3

![Stochastic move illustration](_resources/stochastic-move-illustration.png)

---

### 🎯 Goal in Stochastic RL

- Maximize **expected return**:

$$
\mathbb{E}[R_1 + \gamma R_2 + \gamma^2 R_3 + \ldots]
$$

- The job of a reinforcement learning algorithm:
  - Choose policy \( \pi \)
  - Maximize expected sum of discounted rewards

---

### 🔄 Multiple Reward Sequences Possible

![Stochastic return example](_resources/stochastic-return-example.png)

- Same policy can result in different reward sequences:
  - `000100`, `0000100`, or `0040`

- Return is a **random variable**

---

### 🧮 Modified Bellman Equation

$$
Q^\ast(s, a) = R(s, a) + \gamma \mathbb{E}_{s'}[\max_{a'} Q^\ast(s', a')]
$$

---

### 🧪 Lab: Misstep Probability

- `misstep_prob = 0.1` → Slight loss of control
- `misstep_prob = 0.4` → Larger loss of control, lower Q-values

![Misstep effect on values](_resources/misstep-effect-on-values.png)

---

### 🌍 Real World

- Most environments have:
  - **Larger** or even **continuous** state spaces
- Next: We’ll explore how to handle those.

