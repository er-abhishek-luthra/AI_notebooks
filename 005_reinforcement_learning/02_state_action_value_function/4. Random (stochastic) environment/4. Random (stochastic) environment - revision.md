---
title: Random (stochastic) environment - revision
tags: [revision]
publish: false
---

# Random (stochastic) environment

### Key Concepts:
- **Stochastic environment**: Actions may not lead to the same result every time.
- **Expected return**: Average of sum of discounted rewards over many runs.
- **Bellman equation** is modified to incorporate expected value due to randomness in state transitions.

---

### ğŸš— Mars Rover Example:

- Command â€œleftâ€:
  - 90% chance â†’ state 3
  - 10% chance â†’ state 5

- Command â€œrightâ€:
  - 90% chance â†’ state 5
  - 10% chance â†’ state 3

![Stochastic move illustration](_resources/stochastic-move-illustration.png)

---

### ğŸ¯ Goal in Stochastic RL

- Maximize **expected return**:

$$
\mathbb{E}[R_1 + \gamma R_2 + \gamma^2 R_3 + \ldots]
$$

- The job of a reinforcement learning algorithm:
  - Choose policy \( \pi \)
  - Maximize expected sum of discounted rewards

---

### ğŸ”„ Multiple Reward Sequences Possible

![Stochastic return example](_resources/stochastic-return-example.png)

- Same policy can result in different reward sequences:
  - `000100`, `0000100`, or `0040`

- Return is a **random variable**

---

### ğŸ§® Modified Bellman Equation

$$
Q^\ast(s, a) = R(s, a) + \gamma \mathbb{E}_{s'}[\max_{a'} Q^\ast(s', a')]
$$

---

### ğŸ§ª Lab: Misstep Probability

- `misstep_prob = 0.1` â†’ Slight loss of control
- `misstep_prob = 0.4` â†’ Larger loss of control, lower Q-values

![Misstep effect on values](_resources/misstep-effect-on-values.png)

---

### ğŸŒ Real World

- Most environments have:
  - **Larger** or even **continuous** state spaces
- Next: Weâ€™ll explore how to handle those.

