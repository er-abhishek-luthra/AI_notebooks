---
title: 3. Bellman equation
tags: [blog]
publish: true
---

# Bellman equation

In this module, we explore a central concept in reinforcement learning: the **Bellman Equation**. The Bellman equation allows us to compute the **state-action value function**, denoted as $Q(s, a)$, which provides a way to choose good actions at each state.

We begin with the definition of $Q(s, a)$:

> The return if you:
> - Start in state $s$
> - Take action $a$ (once)
> - Then behave optimally after that

![State-action value setup](_resources/bellman-definition-grid.png)

To describe the Bellman equation, we introduce the following notation:
- $s$: current state
- $a$: action taken in state $s$
- $s'$: new state after taking action $a$
- $a'$: action taken in state $s'$
- $R(s)$: reward of current state

In the given MDP example:
- $R(1) = 100$
- $R(2) = 0$
- $R(6) = 40$

The Bellman equation is:

$$
Q(s, a) = R(s) + \gamma \max_{a'} Q(s', a')
$$

This formula tells us that the value of taking action $a$ in state $s$ is the immediate reward $R(s)$ plus the **discounted** best possible return from the next state $s'$.

Let's look at a worked example.

![Example values in Bellman equation](_resources/bellman-equation-example-values.png)

### Example 1:
Let’s calculate $Q(2, \rightarrow)$:
- Current state $s = 2$
- Action $a = \rightarrow$
- Next state $s' = 3$

Using the Bellman equation:
$$
Q(2, \rightarrow) = R(2) + 0.5 \max_{a'} Q(3, a') = 0 + 0.5 \times 25 = 12.5
$$

### Example 2:
Now let’s calculate $Q(4, \leftarrow)$:
- $s = 4$, $a = \leftarrow$, $s' = 3$

Again,
$$
Q(4, \leftarrow) = R(4) + 0.5 \max_{a'} Q(3, a') = 0 + 0.5 \times 25 = 12.5
$$

![Breakdown of Bellman intuition](_resources/bellman-equation-intuition-diagram.png)

The **intuition** behind this equation is:
- First, you get an immediate reward $R(s)$
- Then, you behave optimally from the next state $s'$
- The value of behaving optimally from $s'$ is captured by $\max_{a'} Q(s', a')$
- The total return is a combination of the two

Mathematically, it reflects a **discounted sum of future rewards**:
$$
Q(s,a) = R_1 + \gamma R_2 + \gamma^2 R_3 + \dots
$$

This can be rewritten using the Bellman equation:
$$
Q(s,a) = R(s) + \gamma \max_{a'} Q(s', a')
$$

![Expanded return from sequence of states](_resources/bellman-equation-expansion-example.png)

#### Terminal States:
In terminal states, the Bellman equation simplifies:
$$
Q(s, a) = R(s)
$$
Because there's no $s'$ to transition to. That’s why in the MDP above:
- $Q(1, \cdot) = 100$
- $Q(6, \cdot) = 40$

To summarize:
- **Bellman equation** breaks down return into:
  - Immediate reward from current state
  - Discounted optimal future reward
- Even if the full derivation seems complex, **applying** the equation correctly yields valid reinforcement learning results.

In the next module, we’ll extend this understanding to **stochastic** Markov Decision Processes, where actions can have **random outcomes**.
