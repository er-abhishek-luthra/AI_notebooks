---
title: state action value function example - revision
tags: [revision]
publish: false
---

# state action value function example

This module demonstrates how the state-action value function $Q(s, a)$ varies based on terminal rewards and the discount factor $\gamma$ in a reinforcement learning setup.

## Key Concepts

- **State-action value function $Q(s, a)$**: Expected return when taking action $a$ in state $s$ and following the optimal policy thereafter.
- **Discount factor $\gamma$**: Determines how future rewards are valued.

## Setup

- States: 6
- Actions: 2
- Terminal Rewards:
  - Left terminal: 100
  - Right terminal: 40
- Intermediate Rewards: 0
- Misstep Probability: 0

## Scenarios

### ðŸ”¸ Case 1: $\gamma = 0.5$

- Moderately values future rewards.
- Optimal policy favors left due to higher discounted return.

![Gamma = 0.5 Q-values and Policy](_resources/gamma-0.5-policy-qsa.png)

---

### ðŸ”¸ Case 2: $\gamma = 0.9$

- Highly values future rewards.
- Optimal policy still prefers left even from state 5 because:
  - $Q(5, \text{left}) = 65.61$
  - $Q(5, \text{right}) = 36.0$
- Future rewards are discounted less heavily.

![Gamma = 0.9 Q-values and Policy](_resources/gamma-0.9-policy-qsa.png)

---

### ðŸ”¸ Case 3: $\gamma = 0.3$

- Very impatient; strongly discounts future rewards.
- Chooses closer reward (right terminal) from state 4 despite being smaller.

![Gamma = 0.3 Q-values and Policy](_resources/gamma-0.3-policy-qsa.png)

---

## Takeaways

- **Higher $\gamma$**: Future rewards are valued more â†’ agent prefers longer paths for bigger rewards.
- **Lower $\gamma$**: Agent becomes short-sighted and prefers closer, even if smaller, rewards.
- **Optimal policy** shifts as $\gamma$ and rewards change.

