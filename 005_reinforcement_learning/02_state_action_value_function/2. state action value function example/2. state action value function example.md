---
title: state action value function example
tags: [blog]
publish: true
---

# state action value function example

In this module, we explore how the **state-action value function**, denoted $Q(s, a)$, changes based on different reward settings and discount factors in a Mars rover example. This interactive lab is designed to build and solidify your **intuition** about reinforcement learning problems.

We start with a basic **Jupyter notebook** where you can experiment with how values of $Q(s, a)$ evolve depending on changes in the environment setup.

We begin by defining:
- `num_states = 6`
- `num_actions = 2`

These values represent the number of states and available actions in our environment. You shouldn't change these parameters.

We then specify:
- `terminal_left_reward = 100`
- `terminal_right_reward = 40`
- `each_step_reward = 0`

These represent the rewards at terminal states and during intermediate steps.

### Case 1: Gamma = 0.5

We use a **discount factor** $\gamma = 0.5$, which means that the agent moderately values future rewards. The probability of a misstep is zero for now.

```python
gamma = 0.5
misstep_prob = 0
```

When we visualize the output, we get the following:

![Q(s, a) and Optimal Policy when Gamma = 0.5](_resources/gamma-0.5-policy-qsa.png)

Here, we observe that the agent prefers to go **left from states 1 to 4**, because it expects a **higher cumulative discounted reward** by doing so. The values in the bottom grid represent $Q(s,a)$ and show the difference in values depending on action choices.

### Case 2: Gamma = 0.9

Now we increase the discount factor to:

```python
gamma = 0.9
```

This makes the Mars rover **less impatient**. It values future rewards more strongly since they are **discounted less heavily**.

![Q(s, a) and Optimal Policy when Gamma = 0.9](_resources/gamma-0.9-policy-qsa.png)

Here, $Q(5, \text{left}) = 65.61$ and $Q(5, \text{right}) = 36.0$. The optimal policy still chooses to go **left**, even from state 5, because the long-term benefit of reaching the higher left terminal reward outweighs the short-term right terminal reward.

This visual helps confirm that **$Q(s, a)$ increases for states further from the terminal state** when $\gamma$ is closer to 1.

### Case 3: Gamma = 0.3

Now let’s set:

```python
gamma = 0.3
```

This makes the rover extremely impatient. It greatly devalues future rewards, preferring more immediate returns.

![Q(s, a) and Optimal Policy when Gamma = 0.3](_resources/gamma-0.3-policy-qsa.png)

As expected, the rover chooses to go **right from state 4** despite the reward being lower, because the discount on the future reward is now so steep that the immediate 40 is more appealing than the heavily discounted 100.

---

By playing with this notebook and varying `gamma` and reward values, you will see how:

- $Q(s, a)$ values evolve
- The **optimal policy** changes
- The **best action (with highest expected return)** can switch depending on these parameters

This activity builds crucial intuition before we dive into the most important equation in reinforcement learning: the **Bellman equation**.

So take your time to experiment with the lab notebook, and then we’ll be ready to tackle Bellman equations next!
